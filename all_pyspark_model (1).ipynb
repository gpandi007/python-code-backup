{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n",
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#STEP-1 Import libraries \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from numpy import int64\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report, accuracy_score  \n",
    "from sklearn.metrics import precision_score, recall_score \n",
    "from sklearn.metrics import f1_score, matthews_corrcoef \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from pyhive import hive\n",
    "#from impala.dbapi import connect\n",
    "from hdfs import InsecureClient\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing\n",
    "from pandas.plotting import scatter_matrix\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql \n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:604: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#STEP-2 Read data from HIVE\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession(sc)\n",
    "hive_context = HiveContext(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    " \n",
    "#df = sqlContext.sql(\"SELECT * FROM sda_hdd_db.ml_smart_data_view\") #per min 1 rec dataset\n",
    "df = sqlContext.sql(\"SELECT * FROM events1.smartdata_hourly_view\") #per hour 1 rec dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-3 fill missing with zero \n",
    "df=df.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the data in desc order \n",
    "df=df.orderBy(desc(\"process_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete #STEP-9 take out only serial number from validation_dataset\n",
    "#validation_dataset1 = pd.DataFrame(df.head(700), columns=df.columns)\n",
    "\n",
    "#final_colums1=dataset.dtypes\n",
    "# Download descriptive analysis\n",
    "#final_colums1=dataset.describe()\n",
    "#validation_dataset1.to_csv(r'E:\\work\\Supermicro\\modeling\\data_Q2_2019\\validation_dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#STEP-4 select columns based on 1. less missing value 2. VIF 4. Correlation\n",
    "# 5. Outliers, skewness, kurtosis 6.EDA 7.Descriptive analysis ect... \n",
    "# columns_to_drop = [\"smart_201_normalized\",\"smart_17_raw\",\"smart_245_raw\",\"smart_201_raw\",\"smart_218_raw\",\"smart_16_raw\",\"smart_170_raw\",\"smart_180_normalized\",\"smart_180_raw\",\"model\",\"serial_number\",\"process_date\",\"date\",\"smart_226_raw\",\"smart_8_normalized\",\"smart_254_normalized\",\"smart_12_raw\",\"smart_192_raw\",\"smart_196_normalized\",\"smart_222_normalized\",\"smart_175_raw\",\"smart_190_normalized\",\"smart_13_raw\",\"smart_177_normalized\",\"smart_190_raw\",\"smart_202_raw\",\"smart_255_raw\", \"smart_15_normalized\", \"smart_234_raw\", \"smart_255_normalized\", \"smart_15_raw\", \"smart_234_normalized\", \"smart_206_normalized\", \"smart_206_raw\", \"smart_248_raw\", \"smart_248_normalized\",\"smart_210_raw\", \"smart_224_raw\", \"smart_18_raw\", \"smart_23_raw\", \"smart_24_raw\", \"smart_179_raw\", \"smart_181_raw\", \"smart_182_raw\", \"smart_251_normalized\", \"smart_250_normalized\", \"smart_254_raw\"]\n",
    "\n",
    "df = df.select(\"serial_number\",\n",
    "    \"capacity_bytes\",\n",
    "\"failure\",\n",
    "\"model\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\")\n",
    "cols = df.columns\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-5 OneHotEncoder   \n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['model','serial_number']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "label_stringIdx = StringIndexer(inputCol = 'failure', outputCol = 'label')\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-6 select numericCols\n",
    "numericCols = [\"capacity_bytes\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-7 Y - Variable selection \n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 37080\n",
      "Test Dataset Count: 15822\n"
     ]
    }
   ],
   "source": [
    "#STEP-8 data split \n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n",
    "train_serial_number=train.select('serial_number')\n",
    "#train_serial_number.show()\n",
    "test_serial_number=test.select('serial_number')\n",
    "#test_serial_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-9 take out only serial number from validation_dataset\n",
    "validation_dataset1 = pd.DataFrame(df.head(700), columns=df.columns)\n",
    "#validation_dataset1 = pd.DataFrame(df.tail(100), columns=df.columns) \n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "validation_dataset1 = sqlContext.createDataFrame(validation_dataset1)\n",
    "validation_serial_number=validation_dataset1.select('serial_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-10 drop serial_number once created different data for serial number from validation_dataset\n",
    "columns_to_drop = [\"serial_number\"]\n",
    "train = train.drop(*columns_to_drop)\n",
    "test = test.drop(*columns_to_drop)\n",
    "df = test.drop(*columns_to_drop)\n",
    "validation_dataset1 = validation_dataset1.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### modeling step ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-11 model 1: LogisticRegression # failure # label\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'failure', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988623435722411 0.9979157017137563 1.0 0.9989567636490089 0.9987506940588562\n"
     ]
    }
   ],
   "source": [
    "#STEP-12 accuracy of the lrModel # failure # label\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = lrModel.transform(test).select(\"failure\", \"prediction\")\n",
    "\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "|index|  serial_number|               model|failure|    Confidence_Level|prediction|         probability|\n",
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "|   26| MK0371YHK3Y4MG|Hitachi HUA723030...|      0|[7.48461295927121...|       0.0|[0.99943865477204...|\n",
      "|   29|       Z1Y45E7R| ST3000NM0033-9ZM178|      0|[7.03207434236296...|       0.0|[0.99911768114236...|\n",
      "|  474|WD-WCC5ZEZ5NDRN|WDC WD500VF4PZ-49...|      0|[7.53089876303461...|       0.0|[0.99946403134354...|\n",
      "|   65| MK0371YHKB65WA|Hitachi HUA723030...|      0|[7.48612037188067...|       0.0|[0.99943949983954...|\n",
      "|  191| STN604MH11MXXB|Hitachi HDE721010...|      0|[7.52502453241129...|       0.0|[0.99946087537707...|\n",
      "|  418| MK0371YHKB457A|Hitachi HUA723030...|      0|[7.49116794868234...|       0.0|[0.99944232030513...|\n",
      "|  541|       Z1Y44P79| ST3000NM0033-9ZM178|      0|[7.30021896633105...|       0.0|[0.99932506497381...|\n",
      "|  558| MK0371YHKB65WA|Hitachi HUA723030...|      0|[7.47868375136283...|       0.0|[0.99943531843779...|\n",
      "|  222| MK0371YHKBGMVA|Hitachi HUA723030...|      0|[7.51727696433540...|       0.0|[0.99945668452825...|\n",
      "|  270|       5QE068NL|         ST3250620AS|      0|[7.27235813812868...|       0.0|[0.99930600955772...|\n",
      "|  293|       6YD1L69W|  ST2000DL003-9VT166|      0|[6.68161032706906...|       0.0|[0.99874781255262...|\n",
      "|  243| MK0351YHG5EBTA|Hitachi HUA723030...|      0|[7.39354043653985...|       0.0|[0.99938516428832...|\n",
      "|  278| MK0331YHGDSGXA|Hitachi HUA723030...|      0|[7.50290344505613...|       0.0|[0.99944882311443...|\n",
      "|  367|WD-WCC5ZEZ5NDRN|WDC WD500VF4PZ-49...|      0|[7.51026589816517...|       0.0|[0.99945286401440...|\n",
      "|  442|WD-WMAY00016492|WDC WD2003FYYS-02...|      0|[7.77987205275144...|       0.0|[0.99958210904431...|\n",
      "|   19| MK0371YHJXMPJA|Hitachi HUA723030...|      0|[7.49374244892461...|       0.0|[0.99944375340746...|\n",
      "|   54|       Z4D07P8Q| ST6000NM0024-1HT17Z|      0|[7.02604384682178...|       0.0|[0.99911234898383...|\n",
      "|  296| MK0371YHKBGUVA|Hitachi HUA723030...|      0|[7.46487388310533...|       0.0|[0.99942747066083...|\n",
      "|    0| MK0271YGJA4WUA|Hitachi HUA723020...|      0|[7.53730506772216...|       0.0|[0.99946745212453...|\n",
      "|  277|WD-WMAY00016492|WDC WD2003FYYS-02...|      0|[7.77988715464106...|       0.0|[0.99958211535257...|\n",
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = lrModel.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#STEP-11 DecisionTreeClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'failure', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "#predictions = dtModel.transform(test)\n",
    "#predictions1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#predictions1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9972905525846703 0.9955901125681792 1.0 0.997790183763666 0.9965112008813809\n"
     ]
    }
   ],
   "source": [
    "#STEP-12 accuracy of the DecisionTreeClassifier \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = dtModel.transform(test).select(\"failure\", \"prediction\")\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+-------+----------------+----------+-----------+\n",
      "|index|  serial_number|               model|failure|Confidence_Level|prediction|probability|\n",
      "+-----+---------------+--------------------+-------+----------------+----------+-----------+\n",
      "|   26| MK0371YHK3Y4MG|Hitachi HUA723030...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|   29|       Z1Y45E7R| ST3000NM0033-9ZM178|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  474| MK0271YGJA4WUA|Hitachi HUA723020...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|   65|       Z1Y45RA5| ST3000NM0033-9ZM178|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  191|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  418|       Z4D07P8Q| ST6000NM0024-1HT17Z|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  541|       Z1Y44P79| ST3000NM0033-9ZM178|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  558|       Z1Y45RA5| ST3000NM0033-9ZM178|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  222| MK0371YHK9UKNA|Hitachi HUA723030...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  270|WD-WCC130857764|WDC WD4000FYYZ-01...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  293|       Z1Y44P79| ST3000NM0033-9ZM178|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  243|WD-WCC5Z0010781|WDC WD500VF4PZ-49...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  278| MK0171YFJMN9AA|Hitachi HUA723020...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  367|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  442| MK0371YHK3Y4MG|Hitachi HUA723030...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|   19| MK0371YHJXMPJA|Hitachi HUA723030...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|   54|WD-WCAVY0102873|WDC WD2002FYPS-01...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  296|WD-WCC5ZLX9VCEY|WDC WD500VF4PZ-49...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|    0|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "|  277| MK0371YHK9UKNA|Hitachi HUA723030...|      0|   [12552.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "+-----+---------------+--------------------+-------+----------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = dtModel.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-11 model 3 RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'failure')\n",
    "rfModel = rf.fit(train)\n",
    "#predictions = rfModel.transform(test)\n",
    "#predictions2=predictions.select('model', 'label', 'rawPrediction', 'prediction', 'probability')\n",
    "#predictions2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9938667807730709 1.0 0.9899696757639375 0.9949595592544835 0.9949848378819688\n"
     ]
    }
   ],
   "source": [
    "#STEP-12 accuracy of the RandomForestClassifier \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = rfModel.transform(test).select(\"failure\", \"prediction\")\n",
    "\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3227.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 326.0 failed 1 times, most recent failure: Lost task 4.0 in stage 326.0 (TID 22383) (sm1.dev.com executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1390, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (2) does not match with length of fields (3)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1390, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (2) does not match with length of fields (3)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-7bb1ea29c90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfinal1_model_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_serial_number_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal1_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mfinal1_model_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal1_model_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Confidence_Level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfinal1_model_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3227.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 326.0 failed 1 times, most recent failure: Lost task 4.0 in stage 326.0 (TID 22383) (sm1.dev.com executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1390, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (2) does not match with length of fields (3)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/smicro/spark/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1411, in verify\n    verify_value(obj)\n  File \"/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1390, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (2) does not match with length of fields (3)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = rfModel.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-11 model 4 GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol = 'features', labelCol = 'failure', maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "\n",
    "#from pyspark.ml.classification import GBTClassifier\n",
    "#gbt = GBTClassifier(maxIter=10)\n",
    "#gbtModel = gbt.fit(train)\n",
    "#predictions = gbtModel.transform(test)\n",
    "#predictions3=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#predictions3.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997860657491264 0.9997666277712952 0.9998833002684094 0.9998249606161387 0.9997582984657045\n"
     ]
    }
   ],
   "source": [
    "#STEP-12 accuracy of the GBTClassifier \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = gbtModel.transform(test).select(\"failure\", \"prediction\")\n",
    "\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "|index|  serial_number|               model|failure|    Confidence_Level|prediction|         probability|\n",
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "|   26| MK0371YHK3Y4MG|Hitachi HUA723030...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|   29|       Z1Y45E7R| ST3000NM0033-9ZM178|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  474| MK0271YGJA4WUA|Hitachi HUA723020...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|   65|       Z1Y45RA5| ST3000NM0033-9ZM178|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  191|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  418|       Z4D07P8Q| ST6000NM0024-1HT17Z|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  541|       Z1Y44P79| ST3000NM0033-9ZM178|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  558|       Z1Y45RA5| ST3000NM0033-9ZM178|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  222| MK0371YHK9UKNA|Hitachi HUA723030...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  270|WD-WCC130857764|WDC WD4000FYYZ-01...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  293|       Z1Y44P79| ST3000NM0033-9ZM178|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  243|WD-WCC5Z0010781|WDC WD500VF4PZ-49...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  278| MK0171YFJMN9AA|Hitachi HUA723020...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  367|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  442| MK0371YHK3Y4MG|Hitachi HUA723030...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|   19| MK0371YHJXMPJA|Hitachi HUA723030...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|   54|WD-WCAVY0102873|WDC WD2002FYPS-01...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  296|WD-WCC5ZLX9VCEY|WDC WD500VF4PZ-49...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|    0|WD-WCAYU8376963|WDC WD5002AALX-00...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "|  277| MK0371YHK9UKNA|Hitachi HUA723030...|      0|[1.32590267922033...|       0.0|[0.93412217565278...|\n",
      "+-----+---------------+--------------------+-------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = gbtModel.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9858523065965797 1.0 0.8178056926374557 0.8997723969616366 0.9089028463187279\n"
     ]
    }
   ],
   "source": [
    "#STEP-12 accuracy of the lsvc  / lsvcModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = lsvcModel.transform(test).select(\"failure\", \"prediction\")\n",
    "\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # lsvcModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+-----+--------------------+----------+\n",
      "|index|     serial_number|               model|label|       rawPrediction|prediction|\n",
      "+-----+------------------+--------------------+-----+--------------------+----------+\n",
      "|   26|    MK0371YHJXMPJA|Hitachi HUA723030...|  0.0|[1.15591572843796...|       0.0|\n",
      "|   29|    MK0371YHKB6XZA|Hitachi HUA723030...|  0.0|[0.35146291852612...|       0.0|\n",
      "|   65|          Z1Y45DJ1| ST3000NM0033-9ZM178|  0.0|[1.22682561455471...|       0.0|\n",
      "|   19|    MK0371YHK3Y4MG|Hitachi HUA723030...|  0.0|[1.15626020100840...|       0.0|\n",
      "|   54|    MK0371YHKB65WA|Hitachi HUA723030...|  0.0|[1.15389461796547...|       0.0|\n",
      "|    0|CVPR223400LU040AGN| INTEL SSDSA2BT040G3|  0.0|[1.21009488313041...|       0.0|\n",
      "|   22|    MK0371YHKAN7BG|Hitachi HUA723030...|  0.0|[1.15486206407390...|       0.0|\n",
      "|    7|    MK0351YHG5EBTA|Hitachi HUA723030...|  0.0|[1.16845546207702...|       0.0|\n",
      "|   77|    MK0371YHKBGUVA|Hitachi HUA723030...|  0.0|[1.15884081220164...|       0.0|\n",
      "|   34|          Z1Y45DJ1| ST3000NM0033-9ZM178|  0.0|[1.22682561455509...|       0.0|\n",
      "|   50|    MK0371YHK3Y4MG|Hitachi HUA723030...|  0.0|[1.15626020100840...|       0.0|\n",
      "|   94|      3511K05DFTMB| TOSHIBA MG04ACA600E|  0.0|[0.86858116917548...|       0.0|\n",
      "|   57|    MK0371YHJXMPJA|Hitachi HUA723030...|  0.0|[1.15591572843796...|       0.0|\n",
      "|   32|      3511K05DFTMB| TOSHIBA MG04ACA600E|  0.0|[0.86858116917548...|       0.0|\n",
      "|   43|    MK0371YHKB457A|Hitachi HUA723030...|  0.0|[1.15450595591666...|       0.0|\n",
      "|   84|    MK0371YHKAN7BG|Hitachi HUA723030...|  0.0|[1.15486206407390...|       0.0|\n",
      "|   31|CVPR223400LU040AGN| INTEL SSDSA2BT040G3|  0.0|[1.21009488313041...|       0.0|\n",
      "|   39|          Z1Y44NLQ| ST3000NM0033-9ZM178|  0.0|[1.24630056080452...|       0.0|\n",
      "|   98|   WD-WCC5Z0012901|WDC WD500VF4PZ-49...|  0.0|[1.62500958201501...|       0.0|\n",
      "|   25|    MK0371YHK9UKNA|Hitachi HUA723030...|  0.0|[1.15332868755522...|       0.0|\n",
      "+-----+------------------+--------------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = lsvcModel.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeDUlEQVR4nO3de5wcZZ3v8c+3ZzIkgYQoCYokkIhxNYhcHPECunhA5CJBBbkoR+WFsrtulBV1D6iLLO55eZSzuOqiGFARlLu7GF9GoyKIcrgkLhchioaLkqgQWAghhGQuv/NHPZ3p6XTPVJKp7kzX9/1Kv6bqqaerf89Mun711OUpRQRmZlZelXYHYGZm7eVEYGZWck4EZmYl50RgZlZyTgRmZiXnRGBmVnJOBGZmJedEYB1F0sOS1kt6RtJfJF0qaae6Oq+X9DNJayWtkfR9SfPq6kyV9G+S/pjW9UCan97kcyXpw5LulbRO0kpJ10rap8j2mo0FJwLrRMdExE7AfsD+wNnVBZJeB/wY+B7wImAOcDdwi6QXpzo9wA3A3sARwFTgdcATwIFNPvOLwBnAh4HnAy8FrgeO3tLgJXVv6XvMtoV8Z7F1EkkPA++PiJ+m+c8De0fE0Wn+F8CvI+KDde/7IbA6It4j6f3A/wb2iohncnzmXOC3wOsi4o4mdW4Cvh0Rl6T596U4D07zASwA/gHoBn4ErIuIj9Ws43vAzyPiAkkvAr4MvBF4BvhCRHxp9N+Q2ebcI7COJWkmcCSwIs1PBl4PXNug+jXAm9P0YcCP8iSB5FBgZbMksAXeBrwGmAdcCZwoSQCSngccDlwlqQJ8n6wns3v6/H+Q9JZt/HwrKScC60TXS1oLPAI8Bnw6lT+f7P/8nxu8589A9fj/Lk3qNLOl9Zv5bET8d0SsB34BBPCGtOx44NaI+BPwamBGRJwXERsj4kHgYuCkMYjBSsiJwDrR2yJiCnAI8DKGNvBPAoPAbg3esxvweJp+okmdZra0fjOPVCciO2Z7FXByKnoX8J00vSfwIklPVV/AJ4AXjEEMVkJOBNaxIuLnwKXA/03z64BbgXc2qH4C2QligJ8Cb5G0Y86PugGYKal3hDrrgMk18y9sFHLd/JXA8ZL2JDtk9N1U/gjwUERMq3lNiYijcsZrNowTgXW6fwPeLGnfNH8W8N50qecUSc+T9C9kVwX9c6pzOdnG9ruSXiapImkXSZ+QtNnGNiJ+D3wFuFLSIZJ6JE2UdJKks1K1u4B3SJos6SXAaaMFHhF3kvVSLgGWRMRTadEdwFpJ/0vSJEldkl4h6dVb/Nsxw4nAOlxErAYuA85J878E3gK8g+y4/h/ILjE9OG3QiYgNZCeMfwv8BHiabOM7Hbi9yUd9GPh34ELgKeAB4O1kJ3UBvgBsBB4FvsXQYZ7RXJFiuaKmTQPAW8kuj32IoWSxc851mg3jy0fNzErOPQIzs5JzIjAzKzknAjOzknMiMDMruXE3uNX06dNj9uzZ7Q7DzGxc+dWvfvV4RMxotGzcJYLZs2ezbNmydodhZjauSPpDs2U+NGRmVnJOBGZmJedEYGZWck4EZmYl50RgZlZyhSUCSd+Q9Jike5ssl6QvSVoh6R5JBxQVi5mZNVdkj+BSsgd/N3MkMDe9Tge+WmAsZmbWRGH3EUTEzZJmj1DlWOCy9CSm2yRNk7RbRIzFI//MbDs2MBj0DQwCEAFBpJ8QEeknMMKyyBYOmx9WLw2s3DcwyFPr++gfCAYje0WQpknzwcDg0HS1fGAwe/UPDk0PiyFND7UjK2NY2VAs9bEO1RuKvdHvpFp46MtfwL6zpo3p3wLae0PZ7tQ8mg9Ymco2SwSSTifrNbDHHnu0JDgbe9Uv2Kaf1f/oNdOD9V+y9EWofik39g/SNzCYfSlT2WD6Ag9EMFjzhd04MJjqD98ABEPvqd1oDMbwssHBbLq6IehLnz246UsdqV3V+dq2Dq9Tu3Doyx4N6g+fb/Q59b/TLXlfdXLtc/081zcwbGM29HcZWu+m31nN76ZvYJCnnu0btkGtfsawMoZvLGuXrd84QP9ggwbZiHadOrHjEkFuEbEQWAjQ29s7Lv/31G/Uar9AfYOD9PUPZhubgcGhjUKTvaHqMmqWDQwGz24cYP3GAdZt7GdD/2D6Yg/f0xkYhDXr+3j06efY0D+w6Utev/dRv8cFtRuMobK+/kHWbexPnzvA+o39rNs4wLMbshiqe12dpKuiTdPVKak6P7QMDa/TqJ42r45SYe376telmjdqpGV1daqlPV1i6qQJVCSkbHlFKar0s6JsXdm0IPvHxO4u5u02ie4u1bxn6P2161RdWSUFssOEClMnTsjqkOqnOKvz1bYMlaf5NM2wZZuvoxpvd5eYNqmHCV0VKoJKJYu72r5KiqsiUakMTUvZ37o71e/uEl0SlcrQ36c2vk1/Vw3FW9+Gap2h9g0vq65zaP3D/55FaWciWAXMqpmfmcoKc8uKx7nxt49x/6NrWb9xYLO9IGo2dIMxfAMMm5fV7vH0DwZrn+tnfd9AzR5mTRd3OzNlh24m9XQBm3+RsrKhDdWmLxYM/0KSfckm93QzuaeLaZN72HGHLib3dDG5p5sduit0VYY2LpWaz6nU/Acf2kjUxFHz5aku765UmNAlJnRl6+1KX9DsZ/YF79JQeU93hR26K6n+8I1S/caqujGotq12IyGJnq7ss7sqaskX06yV2pkIFgELJF1F9mDuNUWeH/jzmvW8+5LsKYP77L4zUyZ2D98A1u15VDcU0Ggjtfn7uitiysRuJvZ0Ndgzqtuowaa9ioq0aeM2oatCd2VoT2ZYTHV7StQt75KYlDbAk3u6mDihMrSXU7enM2ViNzvuMC46g2bWAoVtDSRdCRwCTJe0Evg0MAEgIi4CFgNHASuAZ4FTi4oF4IlnNgLwlXcfwFH77FbkR5mZjStFXjV08ijLA/j7oj6/3kA6UL1Dt++hMzOrVZqtYv9gdqlad1dpmmxmlktptop9A1mPYELFJ/rMzGqVJhFUDw25R2BmNlxptorVuxi73CMwMxumNImgv3poqMuJwMysVnkSQfXQUKU0TTYzy6U0W8Whq4bcIzAzq1WeRDBQ7RE4EZiZ1SpNIqieLJ7gq4bMzIYpzVaxevmorxoyMxuuNImgb9N9BE4EZma1SpMI+quHhnzVkJnZMKXZKlYfjlLxoSEzs2FKkwjMzKwxJwIzs5JzIjAzKzknAjOzknMiMDMrOScCM7OScyIwMys5JwIzs5JzIjAzKzknAjOzknMiMDMrOScCM7OScyIwMys5JwIzs5JzIjAzKzknAjOzknMiMDMrOScCM7OSKzQRSDpC0v2SVkg6q8HyPSTdKOlOSfdIOqrIeMzMbHOFJQJJXcCFwJHAPOBkSfPqqn0KuCYi9gdOAr5SVDxmZtZYkT2CA4EVEfFgRGwErgKOrasTwNQ0vTPwpwLjMTOzBopMBLsDj9TMr0xltc4FTpG0ElgMfKjRiiSdLmmZpGWrV68uIlYzs9Jq98nik4FLI2ImcBRwuaTNYoqIhRHRGxG9M2bMaHmQZmadrMhEsAqYVTM/M5XVOg24BiAibgUmAtMLjMnMzOoUmQiWAnMlzZHUQ3YyeFFdnT8ChwJIejlZIvCxHzOzFiosEUREP7AAWAL8huzqoPsknSdpfqr2UeADku4GrgTeFxFRVExmZra57iJXHhGLyU4C15adUzO9HDioyBjMzGxk7T5ZbGZmbeZEYGZWcqVJBD71YGbWWGkSQZXU7gjMzLYvpUsEZmY2nBOBmVnJORGYmZWcE4GZWcnlSgSSJkn6q6KDMTOz1hs1EUg6BrgL+FGa309S/ZhBZmY2TuXpEZxL9pCZpwAi4i5gTmERmZlZS+VJBH0RsaauzHdnmZl1iDyDzt0n6V1Al6S5wIeB/1dsWGZm1ip5egQfAvYGNgBXAGuAM4oMyszMWidPj+DoiPgk8MlqgaR3AtcWFpWZmbVMnh7B2TnLzMxsHGraI5B0JNkD5XeX9KWaRVOB/qIDMzOz1hjp0NCfgGXAfOBXNeVrgY8UGZSZmbVO00QQEXcDd0u6IiL6WhiTmZm1UJ6TxbMlfRaYB0ysFkbEiwuLyszMWibPyeJvAl8lOy/wJuAy4NtFBmVmZq2TJxFMiogbAEXEHyLiXODoYsMyM7NWyXNoaIOkCvB7SQuAVcBOxYZlZmatkqdHcAYwmWxoiVcB/xN4T5FBmZlZ64zaI4iIpWnyGeBUSV3AScDtRQZmZmat0bRHIGmqpLMl/bukw5VZAKwATmhdiGZmVqSRegSXA08CtwLvBz4BCHh7eiaBmZl1gJESwYsjYh8ASZcAfwb2iIjnWhKZmZm1xEgnizfdTRwRA8BKJwEzs84zUo9gX0lPp2kBk9K8gIiIqYVHZ2ZmhWvaI4iIroiYml5TIqK7ZjpXEpB0hKT7Ja2QdFaTOidIWi7pPklXbG1DzMxs6+S5oWyrpMtMLwTeDKwElkpaFBHLa+rMJXu2wUER8aSkXYuKx8zMGstzQ9nWOhBYEREPRsRG4Crg2Lo6HwAujIgnASLisQLjMTOzBopMBLsDj9TMr0xltV4KvFTSLZJuk3REoxVJOl3SMknLVq9evU1BaZvebWbWeXIlAkl7SjosTU+SNGWMPr8bmAscApwMXCxpWn2liFgYEb0R0Ttjxowx+mgzM4MciUDSB4DrgK+lopnA9TnWvQqYVTM/M5XVWgksioi+iHgI+B1ZYjAzsxbJ0yP4e+Ag4GmAiPg9kOek7lJgrqQ5knrIxidaVFfnerLeAJKmkx0qejBP4GZmNjbyJIIN6WQvAJK6gRjtTRHRDywAlgC/Aa6JiPsknSdpfqq2BHhC0nLgRuDjEfHEljbCzMy2Xp7LR38u6RNkN5S9Gfgg8P08K4+IxcDiurJzaqYDODO9zMysDfL0CM4CVgO/Bv6GbMP+qSKDMjOz1snTI3gbcFlEXFxwLGZm1gZ5egTHAL+TdLmkt6ZzBGZm1iFGTQQRcSrwEuBasmv9H0jDUpuZWQfItXcfEX2Sfkh2tdAkssNF7y8wLjMza5E8N5QdKelS4PfAccAlwAsLjsvMzFokT4/gPcDVwN9ExIaC4zEzsxYbNRFExMmtCMTMzNqjaSKQ9MuIOFjSWobfSewnlJmZdZCmiSAiDk4/x2qkUTMz2w7lOVl8eZ4yMzMbn/LcULZ37Uy6oexVxYRjZmat1jQRSDo7nR94paSn02st8CjwvZZFaGZmhWqaCCLis+n8wPkRMTW9pkTELhFxdgtjNDOzAo101dDLIuK3wLWSDqhfHhH/VWhkZmbWEiPdR3AmcDrwrw2WBfA/ConIzMxaaqTLR09PP9/UunDMzKzV8lw++k5JU9L0pyT9h6T9iw/NzMxaIc/lo/8UEWslHQwcBnwduKjYsMzMrFXyJIKB9PNoYGFE/ADoKS4kMzNrpTyJYJWkrwEnAosl7ZDzfWZmNg7k2aCfACwB3hIRTwHPBz5eZFBmZtY6eR5V+SzwAPAWSQuAXSPix4VHZmZmLZHnqqEzgO8Au6bXtyV9qOjAxlrE6HXMzMoozxPKTgNeExHrACR9DrgV+HKRgRVFUrtDMDPbruQ5RyCGrhwiTXtrambWIfL0CL4J3C7pP9P828juJTAzsw6Q55nFF0i6CTg4FZ0aEXcWGpWZmbXMSKOPvgZYCOwF/Bo4LSKWtyowMzNrjZHOEVwIfAzYBbgA+EJLIjIzs5YaKRFUIuInEbEhIq4FZrQqKDMza52REsE0Se+ovhrMj0rSEZLul7RC0lkj1DtOUkjq3dIGmJnZthnpZPHPgWOazAfwHyOtWFIX2eGlNwMrgaWSFtWfZ0hDXJ8B3L5loZuZ2VgY6cE0p27jug8EVkTEgwCSrgKOBepPOH8G+Bwev8jMrC2KHEV0d+CRmvmVqWyT9CzkWWlo66YknS5pmaRlq1evHvtIzcxKrG3DSUuqkF2N9NHR6kbEwojojYjeGTN8ztrMbCwVmQhWAbNq5memsqopwCuAmyQ9DLwWWOQTxmZmrZVn9NHJkv5J0sVpfq6kt+ZY91JgrqQ5knqAk4BF1YURsSYipkfE7IiYDdwGzI+IZVvVEjMz2yp5egTfBDYAr0vzq4B/Ge1NEdEPLCB7qM1vgGsi4j5J50mav5XxmpnZGMsz6NxeEXGipJMhe1CNco7lHBGLgcV1Zec0qXtInnWamdnYytMj2ChpEtm9A0jai6yHYGZmHSBPj+DTwI+AWZK+AxwEvK/IoMzMrHXyDEP9E0n/RXZVj4AzIuLxwiMzM7OWyHPV0EHAc+mmr2nAJyTtWXRgZmbWGnnOEXwVeFbSvsCZwAPAZYVGZWZmLZMnEfRHRJCNE3RhRFxIdjOYmZl1gDwni9dKOhs4BXhjGhpiQrFhmZlZq+TpEZxIdrnoaRHxF7KhIs4vNCozM2uZPFcN/YVscLjq/B/xOQIzs44x0sPr15JuIqtfBERETC0sKjMza5mRHkzjE8JmZiWQ52QxAJJ2BSZW59MhIjMzG+fy3FA2X9LvgYfInlv8MPDDguMyM7MWyXPV0GfIhpf4XUTMAQ4le3bAuBINT3eYmVmeRNAXEU8AFUmViLgRGLdPEcs1fraZWYnkOUfwlKSdgJuB70h6DFhXbFhmZtYqTXsEkvZIk8cCzwIfIRuO+gHgmOJDMzOzVhipR3A9cEBErJP03Yg4DvhWa8IyM7NWGekcQe3h9BcXHYiZmbXHSIkgmkybmVkHGenQ0L6SnibrGUxK0+AhJszMOspIQ0x0tTIQMzNrjzz3EZiZWQdzIjAzKzknAjOzknMiMDMrOScCM7OScyIwMys5JwIzs5JzIjAzK7lCE4GkIyTdL2mFpLMaLD9T0nJJ90i6QdKeRcZjZmabKywRSOoCLgSOBOYBJ0uaV1ftTqA3Il4JXAd8vqh4zMyssSJ7BAcCKyLiwYjYCFxF9myDTSLixoh4Ns3eBswsMB4zM2ugyESwO/BIzfzKVNbMacAPGy2QdLqkZZKWrV69egxDNDOz7eJksaRTyJ6DfH6j5RGxMCJ6I6J3xowZrQ3OzKzD5Xlm8dZaBcyqmZ+ZyoaRdBjwSeCvI2JDgfGYmVkDRfYIlgJzJc2R1AOcBCyqrSBpf+BrwPyIeKzAWMzMrInCEkFE9AMLgCXAb4BrIuI+SedJmp+qnQ/sBFwr6S5Ji5qszszMClLkoSEiYjGwuK7snJrpw4r8fDMzG912cbLYzMzax4nAzKzknAjMzErOicDMrORKkwgi2h2Bmdn2qTSJoEpqdwRmZtuX0iUCMzMbzonAzKzknAjMzErOicDMrOScCMzMSs6JwMys5JwIzMxKzonAzKzknAjMzErOicDMrOScCMzMSs6JwMys5JwIzMxKzonAzKzknAjMzErOicDMrOScCMzMSs6JwMys5JwIzMxKzonAzKzknAjMzErOicDMrOScCMzMSs6JwMys5JwIzMxKrtBEIOkISfdLWiHprAbLd5B0dVp+u6TZRcZjZmabKywRSOoCLgSOBOYBJ0uaV1ftNODJiHgJ8AXgc0XFY2ZmjRXZIzgQWBERD0bERuAq4Ni6OscC30rT1wGHSlKBMZmZWZ0iE8HuwCM18ytTWcM6EdEPrAF2qV+RpNMlLZO0bPXq1VsVzJzpO3LUPi+k4jxjZjZMd7sDyCMiFgILAXp7e2Nr1nH43i/k8L1fOKZxmZl1giJ7BKuAWTXzM1NZwzqSuoGdgScKjMnMzOoUmQiWAnMlzZHUA5wELKqrswh4b5o+HvhZRGzVHr+ZmW2dwg4NRUS/pAXAEqAL+EZE3CfpPGBZRCwCvg5cLmkF8N9kycLMzFqo0HMEEbEYWFxXdk7N9HPAO4uMwczMRuY7i83MSs6JwMys5JwIzMxKzonAzKzkNN6u1pS0GvjDVr59OvD4GIYzHrjN5eA2l8O2tHnPiJjRaMG4SwTbQtKyiOhtdxyt5DaXg9tcDkW12YeGzMxKzonAzKzkypYIFrY7gDZwm8vBbS6HQtpcqnMEZma2ubL1CMzMrI4TgZlZyXVkIpB0hKT7Ja2QdFaD5TtIujotv13S7DaEOaZytPlMScsl3SPpBkl7tiPOsTRam2vqHScpJI37Sw3ztFnSCelvfZ+kK1od41jL8X97D0k3Sroz/f8+qh1xjhVJ35D0mKR7myyXpC+l38c9kg7Y5g+NiI56kQ15/QDwYqAHuBuYV1fng8BFafok4Op2x92CNr8JmJym/64MbU71pgA3A7cBve2OuwV/57nAncDz0vyu7Y67BW1eCPxdmp4HPNzuuLexzW8EDgDubbL8KOCHgIDXArdv62d2Yo/gQGBFRDwYERuBq4Bj6+ocC3wrTV8HHCqN64cZj9rmiLgxIp5Ns7eRPTFuPMvzdwb4DPA54LlWBleQPG3+AHBhRDwJEBGPtTjGsZanzQFMTdM7A39qYXxjLiJuJns+SzPHApdF5jZgmqTdtuUzOzER7A48UjO/MpU1rBMR/cAaYJeWRFeMPG2udRrZHsV4NmqbU5d5VkT8oJWBFSjP3/mlwEsl3SLpNklHtCy6YuRp87nAKZJWkj3/5EOtCa1ttvT7Pqpx8fB6GzuSTgF6gb9udyxFklQBLgDe1+ZQWq2b7PDQIWS9vpsl7RMRT7UzqIKdDFwaEf8q6XVkTz18RUQMtjuw8aITewSrgFk18zNTWcM6krrJupNPtCS6YuRpM5IOAz4JzI+IDS2KrSijtXkK8ArgJkkPkx1LXTTOTxjn+TuvBBZFRF9EPAT8jiwxjFd52nwacA1ARNwKTCQbnK1T5fq+b4lOTARLgbmS5kjqITsZvKiuziLgvWn6eOBnkc7CjFOjtlnS/sDXyJLAeD9uDKO0OSLWRMT0iJgdEbPJzovMj4hl7Ql3TOT5v309WW8ASdPJDhU92MIYx1qeNv8ROBRA0svJEsHqlkbZWouA96Srh14LrImIP2/LCjvu0FBE9EtaACwhu+LgGxFxn6TzgGURsQj4Oln3cQXZSZmT2hfxtsvZ5vOBnYBr03nxP0bE/LYFvY1ytrmj5GzzEuBwScuBAeDjETFue7s52/xR4GJJHyE7cfy+8bxjJ+lKsmQ+PZ33+DQwASAiLiI7D3IUsAJ4Fjh1mz9zHP++zMxsDHTioSEzM9sCTgRmZiXnRGBmVnJOBGZmJedEYGZWck4Ett2StIuku9LrL5JW1cz3jNFn3JRGtrw7DcvwV1uxjsWSpqXXB2vKXyTpujGIcbak9andyyVdJmnCKO85RNLrt/WzrRycCGy7FRFPRMR+EbEfcBHwhep8RGxMd4WPhXdHxL5kAxGevxVxHpWGcJhGNrJttfxPEXH8GMX4QPo97EN2J+kJo9Q/BHAisFycCGxckXSppIsk3Q58XtK5kj5Ws/xepedLSDpF0h1pT/prkrpGWf3NwEvSHZvnp3X9WtKJaX27Sbo5re9eSW9I5Q+nu3j/D7BXWn5+2pO/N9W5TdLeNXHeJKlX0o7Kxp+/Q9l4+o1GUN0kIgaAO0iDjEk6RtkzNe6U9FNJL0jt/1vgIymWN0iaIem7kpam10Fb8Gu3DudEYOPRTOD1EXFmswppqIETgYPSnvQA8O5R1nsM8GvgHcB+wL7AYcD5yob5fRewJK1vX+CuuvefRdpzj4iP1y27mrQXn9a1Wxru4pNkQ5wcSPbMiPMl7ThCuyYCrwF+lIp+Cbw2IvYnG6L5HyPiYYb3oH4BfDHNvxo4DrhklN+FlUjHDTFhpXBt2jMeyaHAq4ClaUiNSUCzMZa+I2k98DDZEMZnAlemz3hU0s+BV5ONe/ONdHz++oi4awtivgb4MdlwASeQPQcD4HBgfk2vZiKwB/CbuvfvJekuYA7wg4i4J5XPBK5OyaUHeKjJ5x8GzNPQYzemStopIp7ZgjZYh3IisPFoXc10P8N7thPTTwHfioizc6zv3bWD0anJM4oi4mZJbwSOBi6VdEFEXJYn4IhYJekJSa8k66n8bU2cx0XE/aOs4oGI2C8dgrpF0vw0zs6XgQsiYpGkQ8jG5m+kQtZz6IQH9NgY86EhG+8eJnusX/VBNHNS+Q3A8ZJ2Tcuer/zPaf4FcKKkLkkzyB4deEd6/6MRcTHZoZX6Z8WuJRv+upmrgX8Edq7Zo18CfEgp+ygbJbapiHic7BBUNcHtzNAQxO+tqVofy4+peWCLpP1G+hwrFycCG+++Czxf0n3AArLx94mI5cCngB9Lugf4CZD3cX7/CdxD9nzcn5Edd/8L2ZU4d0u6k2yv/ou1b0qjfN6STiQ3uvroOrKRbq+pKfsM2ciS96Q2fCZHfNcDk9PJ6nPJRpT9FfB4TZ3vA2+vniwGPgz0KnvY+XKGeiRmHn3UzKzs3CMwMys5JwIzs5JzIjAzKzknAjOzknMiMDMrOScCM7OScyIwMyu5/w+b5SqYp6XKOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set areaUnderROC: 0.9938394909074526\n"
     ]
    }
   ],
   "source": [
    "# model tuning ROC\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model tuning ROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model tuning # Precision & Recall\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.MultilayerPerceptronClassifier.html\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "mlp = MultilayerPerceptronClassifier(layers=[2, 2, 2], seed=123)\n",
    "mlp.setMaxIter(100)\n",
    "mlp.getMaxIter()\n",
    "mlp.getBlockSize()\n",
    "mlp.setBlockSize(1)\n",
    "model = mlp.fit(train)\n",
    "\n",
    "model.setFeaturesCol(\"features\")\n",
    "\n",
    "model.transform(testDF).select(\"features\", \"prediction\").show()\n",
    "#mlpc=MultilayerPerceptronClassifier(featuresCol=features,labelCol=label,layers = [4,16,2],\\\n",
    "       #                             maxIter=1000,blockSize=8,seed=7,solver=gd)\n",
    "\n",
    "#mlpc=MultilayerPerceptronClassifier(featuresCol='features',labelCol='label',layers = [4,16,2],\\\n",
    "           #                         maxIter=1000,blockSize=8,seed=7,solver=gd)\n",
    "    \n",
    "#from pyspark.ml.classification import MLPClassifier\n",
    "\n",
    "####mlpc=MultilayerPerceptronClassifier(featuresCol='features',labelCol='label')\n",
    "#ann = mlpc.fit(train)\n",
    "\n",
    "\n",
    "#from pyspark.ml.classification import GBTClassifier\n",
    "#gbt = GBTClassifier(maxIter=10)\n",
    "#gbtModel = gbt.fit(train)\n",
    "\n",
    "#from pyspark.ml.classification import RandomForestClassifier\n",
    "#rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "#rfModel = rf.fit(train)\n",
    "#pred = ann.transform(validation_dataset1)\n",
    "#evaluator = MulticlassClassificationEvaluator(labelCol='Class ',predictionCol='prediction',metricName='f1')\n",
    "#ann_f1 = evaluator.evaluate(pred)\n",
    "#ann_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1950.fit.\n: java.util.NoSuchElementException: Failed to find a default value for layers\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:190)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-89edee10f5b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1950.fit.\n: java.util.NoSuchElementException: Failed to find a default value for layers\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:190)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "ann = mlpc.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-12 accuracy of the GBTClassifier \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = ann.transform(test).select(\"failure\", \"prediction\")\n",
    "\n",
    "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
    "\n",
    "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
    "print(acc,precision,recall,f1,auc) # GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-13 predictions with validation data   \n",
    "#STEP-13 predictions with validation data  # failure # label\n",
    "predictions = ann.transform(validation_dataset1)\n",
    "final1=predictions.select('model', 'failure', 'rawPrediction', 'prediction', 'probability')\n",
    "#row_number = final1.count()\n",
    "#row_number\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import Row\n",
    "def zipindexdf(df):\n",
    "    schema_new = df.schema.add(\"index\", LongType(), False)\n",
    "    return df.rdd.zipWithIndex().map(lambda l: list(l[0]) + [l[1]]).toDF(schema_new)\n",
    "\n",
    "validation_serial_number_index = zipindexdf(validation_serial_number)\n",
    "#validation_serial_number_index.show()\n",
    "final1_index = zipindexdf(final1)\n",
    "#final_index.show()\n",
    "final1_model_results = validation_serial_number_index.join(final1_index, \"index\", \"inner\")\n",
    "final1_model_results = final1_model_results.withColumnRenamed(\"rawPrediction\", \"Confidence_Level\")\n",
    "final1_model_results.show()\n",
    "\n",
    "# STEP-14 Download the predictions results\n",
    "pandasDF = final1_model_results.toPandas()\n",
    "#pandasDF.label.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.failure.replace((0, 1), ('Active', 'fail'), inplace=True)\n",
    "pandasDF.prediction.replace((0, 1), ('Active', 'Predicted_to_be_fail'), inplace=True)\n",
    "\n",
    "pandasDF['Confidence_Level'] = pandasDF['Confidence_Level'].astype('str')\n",
    "pandasDF['probability'] = pandasDF['probability'].astype('str')\n",
    "Confidence_Level=pandasDF['Confidence_Level'].str.split(',', expand=True)\n",
    "pandasDF = pandasDF.join(Confidence_Level).drop(1, axis=1)\n",
    "pandasDF = pandasDF.drop(['Confidence_Level'], 1)\n",
    "pandasDF.rename(columns = {0:'Confidence_Level'}, inplace = True)\n",
    "probability1=pandasDF['probability'].str.split(',', expand=True)\n",
    "probability1.rename(columns = {0:'Active_Probability'}, inplace = True)\n",
    "probability1.rename(columns = {1:'Failure_Probability'}, inplace = True)\n",
    "pandasDF = pandasDF.join(probability1)\n",
    "pandasDF = pandasDF.drop(['probability'], 1)\n",
    "pandasDF = pandasDF.drop(['index'], 1)\n",
    "pandasDF.rename(columns = {'serial_number':'Serial_Number'}, inplace = True)\n",
    "pandasDF.rename(columns = {'model':'Model'}, inplace = True)\n",
    "pandasDF.rename(columns = {'label':'Actaul'}, inplace = True)\n",
    "pandasDF.rename(columns = {'prediction':'Prediction'}, inplace = True)\n",
    "pandasDF=pandasDF.replace('\\]','',regex=True).astype(str)\n",
    "pandasDF=pandasDF.replace('\\[','',regex=True).astype(str)\n",
    "\n",
    "#pandasDF=pandasDF.groupby(['Serial_Number']).min()\n",
    "\n",
    "pandasDF.sort_values(by=['Prediction'], inplace=True, ascending=False)\n",
    "pandasDF.sort_values(by=['Active_Probability'], inplace=True)\n",
    "# download csv file \n",
    "pandasDF.to_csv(r'E:\\home\\smicro\\ML-Ganesh\\modelPrediction\\Prediction_Results.csv')\n",
    "# download excel file \n",
    "#pandasDF = final1_model_results.toPandas()\n",
    "writer = pd.ExcelWriter(\"Prediction_Results_ML.xlsx\")\n",
    "pandasDF.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
