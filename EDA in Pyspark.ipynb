{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75211ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n",
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#STEP-1 Import libraries \n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from numpy import int64\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report, accuracy_score  \n",
    "from sklearn.metrics import precision_score, recall_score \n",
    "from sklearn.metrics import f1_score, matthews_corrcoef \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from pyhive import hive\n",
    "#from impala.dbapi import connect\n",
    "from hdfs import InsecureClient\n",
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing\n",
    "from pandas.plotting import scatter_matrix\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql \n",
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31aeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-2 Read data from HIVE\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark import SparkConf\n",
    "spark = SparkSession(sc)\n",
    "hive_context = HiveContext(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "#bank1 = sqlContext.table(\"sda_hdd_db.sda_hdd_all\")\n",
    "dataset = sqlContext.sql(\"SELECT * FROM sda_hdd_db.ml_smart_data_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7357fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert catogorical colum as dable \n",
    "for col in dataset.columns:\n",
    "    dataset = dataset.withColumn(col,dataset[col].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212945f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-4 select columns based on 1. less missing value 2. VIF 4. Correlation\n",
    "# 5. Outliers, skewness, kurtosis 6.EDA 7.Descriptive analysis ect... \n",
    "# columns_to_drop = [\"smart_201_normalized\",\"smart_17_raw\",\"smart_245_raw\",\"smart_201_raw\",\"smart_218_raw\",\"smart_16_raw\",\"smart_170_raw\",\"smart_180_normalized\",\"smart_180_raw\",\"model\",\"serial_number\",\"process_date\",\"date\",\"smart_226_raw\",\"smart_8_normalized\",\"smart_254_normalized\",\"smart_12_raw\",\"smart_192_raw\",\"smart_196_normalized\",\"smart_222_normalized\",\"smart_175_raw\",\"smart_190_normalized\",\"smart_13_raw\",\"smart_177_normalized\",\"smart_190_raw\",\"smart_202_raw\",\"smart_255_raw\", \"smart_15_normalized\", \"smart_234_raw\", \"smart_255_normalized\", \"smart_15_raw\", \"smart_234_normalized\", \"smart_206_normalized\", \"smart_206_raw\", \"smart_248_raw\", \"smart_248_normalized\",\"smart_210_raw\", \"smart_224_raw\", \"smart_18_raw\", \"smart_23_raw\", \"smart_24_raw\", \"smart_179_raw\", \"smart_181_raw\", \"smart_182_raw\", \"smart_251_normalized\", \"smart_250_normalized\", \"smart_254_raw\"]\n",
    "\n",
    "input_cols = dataset.select(\"capacity_bytes\",\n",
    "\"failure\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\")\n",
    "cols = df.columns\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bab9961",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'DataFrame' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2c8897a71e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# impute the null value with mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimputed_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'f_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmissingValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimputed_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimpute_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'DataFrame' has no len()"
     ]
    }
   ],
   "source": [
    "# impute the null value with mean\n",
    "imputed_col = ['f_{}'.format(i+1) for i in range(len(input_cols))]\n",
    "model = Imputer(strategy='mean',missingValue=None,inputCols=input_cols,outputCols=imputed_col).fit(dataset)\n",
    "impute_data = model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "748cf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing \n",
    "dataset=df\n",
    "import pyspark.sql.functions as F\n",
    "def count_missings(spark_df,sort=True):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    dataset = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        return dataset.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "\n",
    "    return dataset\n",
    "missings=count_missings(dataset)\n",
    "# download missing \n",
    "#missings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "writer = pd.ExcelWriter(\"missings_info_pyspark.xlsx\")\n",
    "missings.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "337ee1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-3 fill missing with zero \n",
    "df=df.na.fill(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "853da647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hist\n",
      "  Downloading hist-2.4.0-py3-none-any.whl (31 kB)\n",
      "Collecting histoprint>=1.6\n",
      "  Downloading histoprint-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/smicro/.local/lib/python3.6/site-packages (from hist) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /home/smicro/.local/lib/python3.6/site-packages (from hist) (3.7.4.3)\n",
      "Collecting boost-histogram~=1.1.0\n",
      "  Downloading boost_histogram-1.1.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.5 MB)\n",
      "     |████████████████████████████████| 5.5 MB 15.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from boost-histogram~=1.1.0->hist) (0.8)\n",
      "Collecting uhi>=0.2.1\n",
      "  Downloading uhi-0.3.1-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.6/site-packages (from histoprint>=1.6->hist) (7.1.2)\n",
      "Installing collected packages: uhi, histoprint, boost-histogram, hist\n",
      "Successfully installed boost-histogram-1.1.0 hist-2.4.0 histoprint-2.2.1 uhi-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71ee24c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   990.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
       "             0.,  15584.,      0.,      0.,  15584., 826857.]),\n",
       " array([  0,   5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,\n",
       "         65,  70,  75,  80,  85,  90,  95, 100]),\n",
       " <BarContainer object of 20 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram \n",
    "import pandas as pd\n",
    "import pyspark.sql as sparksql\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "#df.hist(column = 'smart_12_normalized')\n",
    "\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hist(ax, df.select('smart_12_normalized'), bins = 20, color=['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37204e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-72b1b220fb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_1_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'smart_12_normalized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "dataset.smart_1_normalized.plot.density(color='green')\n",
    "plt.title('smart_12_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f079005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-4 select columns based on 1. less missing value 2. VIF 4. Correlation\n",
    "# 5. Outliers, skewness, kurtosis 6.EDA 7.Descriptive analysis ect... \n",
    "# columns_to_drop = [\"smart_201_normalized\",\"smart_17_raw\",\"smart_245_raw\",\"smart_201_raw\",\"smart_218_raw\",\"smart_16_raw\",\"smart_170_raw\",\"smart_180_normalized\",\"smart_180_raw\",\"model\",\"serial_number\",\"process_date\",\"date\",\"smart_226_raw\",\"smart_8_normalized\",\"smart_254_normalized\",\"smart_12_raw\",\"smart_192_raw\",\"smart_196_normalized\",\"smart_222_normalized\",\"smart_175_raw\",\"smart_190_normalized\",\"smart_13_raw\",\"smart_177_normalized\",\"smart_190_raw\",\"smart_202_raw\",\"smart_255_raw\", \"smart_15_normalized\", \"smart_234_raw\", \"smart_255_normalized\", \"smart_15_raw\", \"smart_234_normalized\", \"smart_206_normalized\", \"smart_206_raw\", \"smart_248_raw\", \"smart_248_normalized\",\"smart_210_raw\", \"smart_224_raw\", \"smart_18_raw\", \"smart_23_raw\", \"smart_24_raw\", \"smart_179_raw\", \"smart_181_raw\", \"smart_182_raw\", \"smart_251_normalized\", \"smart_250_normalized\", \"smart_254_raw\"]\n",
    "\n",
    "df = df.select(\"capacity_bytes\",\n",
    "\"failure\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\")\n",
    "cols = df.columns\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8f7aedf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1531.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3045/1766481824.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$1676/620990444.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1677/623478786.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1685/167639019.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1678/2073088127.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-939a1c0e377e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kde'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \"\"\"\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1531.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:373)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:369)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:369)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:390)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3045/1766481824.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset$$Lambda$1676/620990444.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.Dataset$$Lambda$1677/623478786.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1685/167639019.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1678/2073088127.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n"
     ]
    }
   ],
   "source": [
    "# scatter_matrix\n",
    "pdf = df.toPandas()\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "stuff = scatter_matrix(pdf, alpha=0.7, figsize=(6, 6), diagonal='kde', color=pdf.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b6f01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder  OneHotEncoderEstimator, \n",
    "from pyspark.ml.feature import OneHotEncoder,StringIndexer, VectorAssembler\n",
    "# OneHotEncoder  OneHotEncoderEstimator, \n",
    "categoricalColumns = ['model']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "#label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "#stages += [label_stringIdx]\n",
    "numericCols = [\"capacity_bytes\",\n",
    "\"failure\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "727b9018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smicro/.local/lib/python3.6/site-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# Correlation matrix\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13285484",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "cor_np = matrix.collect()[0][matrix.columns[0]].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c6ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, corr_columns, method='pearson'):\n",
    "    vector_col = \"corr_features\"\n",
    "    assembler = VectorAssembler(inputCols=corr_columns, outputCol=vector_col)\n",
    "    df_vector = assembler.transform(df).select(vector_col)\n",
    "    matrix = Correlation.corr(df_vector, vector_col, method)\n",
    "    result = matrix.collect()[0][\"pearson({})\".format(vector_col)].values\n",
    "    return pd.DataFrame(result.reshape(-1, len(corr_columns)), columns=corr_columns, index=corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "818a7723",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-07137c24e558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"describe_pyspark.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdescribe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Sheet1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1660\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_excel'"
     ]
    }
   ],
   "source": [
    "#describe\n",
    "describe=df.describe()\n",
    "#missings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "writer = pd.ExcelWriter(\"describe_pyspark.xlsx\")\n",
    "describe.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ab41bba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1542.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 73 in stage 5.0 failed 1 times, most recent failure: Lost task 73.0 in stage 5.0 (TID 374) (sm1.dev.com executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3325/1254586831: (struct<capacity_bytes_double_VectorAssembler_0549c0629cef:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3325/1254586831: (struct<capacity_bytes_double_VectorAssembler_0549c0629cef:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2baef687b1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Fitting pipeline on dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_Scaled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_Scaled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_Vect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;31m#print(\"After Scaling :\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1542.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 73 in stage 5.0 failed 1 times, most recent failure: Lost task 73.0 in stage 5.0 (TID 374) (sm1.dev.com executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3325/1254586831: (struct<capacity_bytes_double_VectorAssembler_0549c0629cef:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3325/1254586831: (struct<capacity_bytes_double_VectorAssembler_0549c0629cef:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:87)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# scalling\n",
    "#print(\"Before Scaling :\")\n",
    "#dataset.show(5)\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# UDF for converting column type from vector to double type\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "# Iterating over columns to be scaled\n",
    "for i in [\"capacity_bytes\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\"]:\n",
    "    # VectorAssembler Transformation - Converting column to vector type\n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "    # Fitting pipeline on dataframe\n",
    "    dataset = pipeline.fit(dataset).transform(dataset).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "#print(\"After Scaling :\")\n",
    "#dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aaa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop after scalling \n",
    "columns_to_drop = [\"capacity_bytes\",\n",
    "\"smart_1_normalized\",\n",
    "\"smart_1_raw\",\n",
    "\"smart_2_normalized\",\n",
    "\"smart_2_raw\",\n",
    "\"smart_3_normalized\",\n",
    "\"smart_3_raw\",\n",
    "\"smart_4_normalized\",\n",
    "\"smart_4_raw\",\n",
    "\"smart_5_normalized\",\n",
    "\"smart_5_raw\",\n",
    "\"smart_7_normalized\",\n",
    "\"smart_7_raw\",\n",
    "\"smart_8_raw\",\n",
    "\"smart_9_normalized\",\n",
    "\"smart_9_raw\",\n",
    "\"smart_10_normalized\",\n",
    "\"smart_10_raw\",\n",
    "\"smart_11_normalized\",\n",
    "\"smart_11_raw\",\n",
    "\"smart_12_normalized\",\n",
    "\"smart_13_normalized\",\n",
    "\"smart_16_normalized\",\n",
    "\"smart_17_normalized\",\n",
    "\"smart_18_normalized\",\n",
    "\"smart_22_normalized\",\n",
    "\"smart_22_raw\",\n",
    "\"smart_23_normalized\",\n",
    "\"smart_24_normalized\",\n",
    "\"smart_168_normalized\",\n",
    "\"smart_168_raw\",\n",
    "\"smart_170_normalized\",\n",
    "\"smart_173_normalized\",\n",
    "\"smart_173_raw\",\n",
    "\"smart_174_normalized\",\n",
    "\"smart_174_raw\",\n",
    "\"smart_175_normalized\",\n",
    "\"smart_177_raw\",\n",
    "\"smart_179_normalized\",\n",
    "\"smart_181_normalized\",\n",
    "\"smart_182_normalized\",\n",
    "\"smart_183_normalized\",\n",
    "\"smart_183_raw\",\n",
    "\"smart_184_normalized\",\n",
    "\"smart_184_raw\",\n",
    "\"smart_187_normalized\",\n",
    "\"smart_187_raw\",\n",
    "\"smart_188_normalized\",\n",
    "\"smart_188_raw\",\n",
    "\"smart_189_normalized\",\n",
    "\"smart_189_raw\",\n",
    "\"smart_191_normalized\",\n",
    "\"smart_191_raw\",\n",
    "\"smart_192_normalized\",\n",
    "\"smart_193_normalized\",\n",
    "\"smart_193_raw\",\n",
    "\"smart_194_normalized\",\n",
    "\"smart_194_raw\",\n",
    "\"smart_195_normalized\",\n",
    "\"smart_195_raw\",\n",
    "\"smart_196_raw\",\n",
    "\"smart_197_normalized\",\n",
    "\"smart_197_raw\",\n",
    "\"smart_198_normalized\",\n",
    "\"smart_198_raw\",\n",
    "\"smart_199_normalized\",\n",
    "\"smart_199_raw\",\n",
    "\"smart_200_normalized\",\n",
    "\"smart_200_raw\",\n",
    "\"smart_202_normalized\",\n",
    "\"smart_210_normalized\",\n",
    "\"smart_218_normalized\",\n",
    "\"smart_220_normalized\",\n",
    "\"smart_220_raw\",\n",
    "\"smart_222_raw\",\n",
    "\"smart_223_normalized\",\n",
    "\"smart_223_raw\",\n",
    "\"smart_224_normalized\",\n",
    "\"smart_225_normalized\",\n",
    "\"smart_225_raw\",\n",
    "\"smart_226_normalized\",\n",
    "\"smart_231_normalized\",\n",
    "\"smart_231_raw\",\n",
    "\"smart_232_normalized\",\n",
    "\"smart_232_raw\",\n",
    "\"smart_233_normalized\",\n",
    "\"smart_233_raw\",\n",
    "\"smart_235_normalized\",\n",
    "\"smart_235_raw\",\n",
    "\"smart_240_normalized\",\n",
    "\"smart_240_raw\",\n",
    "\"smart_241_normalized\",\n",
    "\"smart_241_raw\",\n",
    "\"smart_242_normalized\",\n",
    "\"smart_242_raw\",\n",
    "\"smart_245_normalized\",\n",
    "\"smart_247_normalized\",\n",
    "\"smart_247_raw\",\n",
    "\"smart_250_raw\",\n",
    "\"smart_251_raw\",\n",
    "\"smart_252_normalized\",\n",
    "\"smart_252_raw\",\n",
    "\"smart_160_normalized\",\n",
    "\"smart_160_raw\",\n",
    "\"smart_161_normalized\",\n",
    "\"smart_161_raw\",\n",
    "\"smart_163_normalized\",\n",
    "\"smart_163_raw\",\n",
    "\"smart_164_normalized\",\n",
    "\"smart_164_raw\",\n",
    "\"smart_165_normalized\",\n",
    "\"smart_165_raw\",\n",
    "\"smart_166_normalized\",\n",
    "\"smart_166_raw\",\n",
    "\"smart_167_normalized\",\n",
    "\"smart_167_raw\",\n",
    "\"smart_169_normalized\",\n",
    "\"smart_169_raw\",\n",
    "\"smart_176_normalized\",\n",
    "\"smart_176_raw\",\n",
    "\"smart_178_normalized\",\n",
    "\"smart_178_raw\"]\n",
    "dataset = dataset.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.printSchema()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e255298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 count missing value# import sql function pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# null values in each column\n",
    "data_agg = dataset.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in dataset.columns])\n",
    "data_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4819ec37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_excel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-82d77ef75eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"describe_pyspark.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdescribe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Sheet1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_excel'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "writer = pd.ExcelWriter(\"describe_pyspark.xlsx\")\n",
    "describe.to_excel(excel_writer=writer, sheet_name='Sheet1', na_rep=\"\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('model').describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "351da46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b917b6f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fb657489c66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcalc_vif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1660\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "calc_vif(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baeb2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
